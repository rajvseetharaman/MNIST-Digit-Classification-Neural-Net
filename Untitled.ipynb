{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return(1/(1+np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    a=np.array(e_x / e_x.sum())\n",
    "    op=np.argmax(a)\n",
    "    return int(op) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X=mnist.train.images.T[:,:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=mnist.train.labels[:,:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1=[]\n",
    "for i in range(Y.shape[0]):\n",
    "    for j in range(Y.shape[1]):\n",
    "        if Y[i,j]==1:\n",
    "            Y1.append(j)\n",
    "Y1=np.array(Y1).reshape(1,len(Y1))[:,:2000] \n",
    "Y1=np.where(Y1>=4,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_size(X,Y):\n",
    "    n_x=X.shape[0]\n",
    "    n_h=10\n",
    "    n_y=Y.shape[0]\n",
    "    return n_x,n_h,n_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(2)\n",
    "    w1=np.random.randn(n_h,n_x)*0.01\n",
    "    b1=np.zeros((n_h,1))\n",
    "    w2=np.random.randn(n_y,n_h)*0.01\n",
    "    b2=np.zeros((n_y,1))\n",
    "    assert (w1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (w2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    return {'W1':w1,'b1':b1,'W2':w2,'b2':b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
    "    Z1=np.dot(parameters['W1'],X)+parameters['b1']\n",
    "    A1=np.tanh(Z1)\n",
    "    Z2=np.dot(parameters['W2'],A1)+parameters['b2']\n",
    "    A2=sigmoid(Z2)\n",
    "    cache={'Z1':Z1,'A1':A1,'Z2':Z2,'A2':A2}\n",
    "    return A2,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    m=Y.shape[1]\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    ### END CODE HERE ###\n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m=X.shape[1]\n",
    "    dZ2=cache['A2']-Y\n",
    "    dW2=(1/m)* np.dot(dZ2,cache['A1'].T)\n",
    "    db2=(1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1=np.multiply(np.dot(parameters['W2'].T,dZ2) , (1 - np.power(cache['A1'], 2))) \n",
    "    dW1=(1/m)*np.dot(dZ1,X.T)\n",
    "    db1=(1/m)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate=1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    parameters['W1']=parameters['W1']-learning_rate*grads['dW1']\n",
    "    parameters['W2']=parameters['W2']-learning_rate*grads['dW2']\n",
    "    parameters['b1']=parameters['b1']-learning_rate*grads['db1']\n",
    "    parameters['b2']=parameters['b2']-learning_rate*grads['db2']\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations=10000, print_cost=False,learning_rate=1.2):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    parameters = initialize_parameters(X.shape[0], n_h, Y.shape[0])\n",
    "    for i in range(num_iterations):\n",
    "        A2,cache=forward_propagation(X,parameters)\n",
    "        grads=backward_propagation(parameters, cache, X, Y)\n",
    "        parameters=update_parameters(parameters, grads, learning_rate)\n",
    "        if i%1000 == 0 and print_cost:\n",
    "            print('Cost: ', compute_cost(A2,Y,parameters))\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    A2,cache=forward_propagation(X, parameters)\n",
    "    predictions=np.where(A2>0.5,1,0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  0.693750378551\n",
      "Cost:  0.00241050912671\n",
      "Cost:  0.000886710833574\n",
      "Cost:  0.00052151091139\n",
      "Cost:  0.00036537763455\n",
      "Cost:  0.000279938409862\n",
      "Cost:  0.000226210797807\n",
      "Cost:  0.000189389063613\n",
      "Cost:  0.000162626592664\n",
      "Cost:  0.000142326719253\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(X, Y1, n_h = 20, num_iterations=10000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=predict(parameters,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=mnist.test.images.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 10000)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test=mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1_test=[]\n",
    "for i in range(Y_test.shape[0]):\n",
    "    for j in range(Y_test.shape[1]):\n",
    "        if Y_test[i,j]==1:\n",
    "            Y1_test.append(j)\n",
    "Y1_test=np.array(Y1_test).reshape(1,len(Y1_test))\n",
    "Y1_test=np.where(Y1_test>=4,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10000)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred=predict(parameters,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9444"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pred==Y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial=X_test[:,200].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13c96536d30>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB4Ii/EBQhB8IivADQf0f3WsDs8CDweAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13c964fefd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example=X_test[:,200].reshape(784,1)\n",
    "predict(parameters,example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 3, 4, ..., 5, 1, 9]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
